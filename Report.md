# Report

## Deep Q-Learning
Q-learning is a very popular algorithm for reinforcement learning, but it has various limitations. Just like its siblings, Sarsa and Expected Sarsa, it has difficulties in dealing with continous state spaces. One way to overcome this is to, discard the old tabular rapresentation of the Q-function and, instead, try to approximate it directly, which can be done through function approximators.
The use of neural networks as these approximators seems promising at first, except for the fact that they don't want to work with these temporal differnece learning methods we are using. If we directly apply them to the Q-learning algorithm, we find that this new learning method is extremely unstable. Because we are now using neural netowrks, when we update the Q-function at one state-action pair, the neural network is going to change the value of all the other state-action pairs nearby as well. This makes it hard for us to utilize current predictions for updating the network (temporal difference learning), because every time we update the netowrk, our target changes as well creating a sort of constantly changing target which is unstable.
One way to address this problem of constantly changing targets called Fixed Q-Targets, is to 'fix' the target at one point for a while and update it less frequently than the normal network.
Another way to stabilize the learning of the agent, even though it doesn't address the same problem describe above, is a technique called experience replay. In experience replay, instead of learning the current experience on policy, the agent stores the experiences it had in a replay buffer from which it then samples a minibatch uniformly at random for training. This allows for the harmful correlations between subsequent experiences to be broken and thus results in better learning.
